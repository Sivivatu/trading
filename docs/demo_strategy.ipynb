{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "The libraries listed in the example dataset. Not all appear to be avaiable on anaconda. need to decide what they are used for, if there is an alternative and, if not, create a private package and publish to anaconda.\n",
    "\n",
    "- numpy\n",
    "- pandas\n",
    "- math\n",
    "- sklearn, sklearn.preprocessing\n",
    "- datetime\n",
    "- os\n",
    "- matplotlib.pyplot\n",
    "- tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn, sklearn.preprocessing\n",
    "import os\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the dataset\n",
    "In this model, we are going to use daily OHLC data for the stock of “RELIANCE” trading on NSE for the time period from 1st January 1996 to 30 Sep 2018. We import our dataset.CSV file named ‘RELIANCE.NS.csv’ saved in the personal drive in your computer. This is done using the pandas library, and the data is stored in a dataframe named dataset. We then drop the missing values in the dataset using the dropna() function. We choose only the OHLC data from this dataset, which would also contain the Date, Adjusted Close and Volume data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-01-01</th>\n",
       "      <td>16.009800</td>\n",
       "      <td>16.095699</td>\n",
       "      <td>15.904300</td>\n",
       "      <td>16.0683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-02</th>\n",
       "      <td>16.029301</td>\n",
       "      <td>16.107401</td>\n",
       "      <td>15.826200</td>\n",
       "      <td>15.9434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-03</th>\n",
       "      <td>16.205000</td>\n",
       "      <td>16.943001</td>\n",
       "      <td>16.029301</td>\n",
       "      <td>16.0644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-04</th>\n",
       "      <td>15.912100</td>\n",
       "      <td>15.962900</td>\n",
       "      <td>15.701300</td>\n",
       "      <td>15.9160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>15.853600</td>\n",
       "      <td>15.853600</td>\n",
       "      <td>15.670000</td>\n",
       "      <td>15.8067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low    Close\n",
       "Date                                                \n",
       "1996-01-01  16.009800  16.095699  15.904300  16.0683\n",
       "1996-01-02  16.029301  16.107401  15.826200  15.9434\n",
       "1996-01-03  16.205000  16.943001  16.029301  16.0644\n",
       "1996-01-04  15.912100  15.962900  15.701300  15.9160\n",
       "1996-01-05  15.853600  15.853600  15.670000  15.8067"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"raw_data/RELIANCE.NS.csv\", index_col = 0)\n",
    "df_stock = dataset.dropna()\n",
    "df_stock = df_stock[['Open', 'High', 'Low', 'Close']]\n",
    "df_stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling data\n",
    "Before we split the data set we have to standardize the dataset. This process makes the mean of all the input features equal to zero and also converts their variance to 1. This ensures that there is no bias while training the model due to the different scales of all input features. If this is not done the neural network might get confused and give a higher weight to those features which have a higher average value than others. Also, most common activation functions of the network’s neurons such as tanh or sigmoid are defined on the [-1, 1] or [0, 1] interval respectively. Nowadays, rectified linear unit (ReLU) activations are commonly used activations which are unbounded on the axis of possible activation values. However, we will scale both the inputs and targets. We will do Scaling using sklearn’s MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-01-01</th>\n",
       "      <td>0.003114</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>0.003110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-02</th>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.003080</td>\n",
       "      <td>0.003015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-03</th>\n",
       "      <td>0.003263</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.003107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-04</th>\n",
       "      <td>0.003039</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>0.002994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>0.002910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open      High       Low     Close\n",
       "Date                                              \n",
       "1996-01-01  0.003114  0.002687  0.003141  0.003110\n",
       "1996-01-02  0.003128  0.002696  0.003080  0.003015\n",
       "1996-01-03  0.003263  0.003331  0.003238  0.003107\n",
       "1996-01-04  0.003039  0.002587  0.002982  0.002994\n",
       "1996-01-05  0.002994  0.002503  0.002958  0.002910"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_data(df):\n",
    "    min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    df['Open'] = min_max_scaler.fit_transform(df.Open.values.reshape(-1,1))\n",
    "    df['High'] = min_max_scaler.fit_transform(df.High.values.reshape(-1,1))\n",
    "    df['Low'] = min_max_scaler.fit_transform(df.Low.values.reshape(-1,1))\n",
    "    df['Close'] = min_max_scaler.fit_transform(df.Close.values.reshape(-1,1))\n",
    "    return df\n",
    "df_stock_norm = normalize_data(df_stock)\n",
    "df_stock_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset and Building X & Y\n",
    "Next, we split the whole dataset into train, valid and test data. Then we can build X & Y. So we will get x_train, y_train, x_valid, y_valid, x_test & y_test. This is a crucial part. Please remember we are not simply slicing the data set like the previous project. Here we are giving sequence length as 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (4497, 19, 4)\n",
      "y_train.shape: (4497, 4)\n",
      "x_valid.shape: (562, 19, 4)\n",
      "y_valid.shape: (562, 4)\n",
      "x_test.shape: (562, 19, 4)\n",
      "y_test.shape: (562, 4)\n"
     ]
    }
   ],
   "source": [
    "# Splitting into Train, Validate & test data\n",
    "valid_set_size_percentage = 10\n",
    "test_set_size_percentage = 10\n",
    "seq_len = 20 # taken sequence length as 20\n",
    "\n",
    "def load_data(stock, seq_len):\n",
    "    data_raw = stock.values\n",
    "    data = []\n",
    "    for index in range(len(data_raw) - seq_len):\n",
    "        data.append(data_raw[index: index + seq_len])\n",
    "    data = np.array(data)\n",
    "    valid_set_size = int(np.round(valid_set_size_percentage/100*data.shape[0]))\n",
    "    test_set_size = int(np.round(test_set_size_percentage/100*data.shape[0]))\n",
    "    train_set_size = data.shape[0] - (valid_set_size + test_set_size)\n",
    "    x_train = data[:train_set_size, :-1, :]\n",
    "    y_train = data[:train_set_size, -1, :]\n",
    "    x_valid = data[train_set_size: train_set_size + valid_set_size, :-1, :]\n",
    "    y_valid = data[train_set_size: train_set_size + valid_set_size, -1, :]\n",
    "    x_test = data[train_set_size + valid_set_size:, :-1, :]\n",
    "    y_test = data[train_set_size + valid_set_size:, -1, :]\n",
    "    return[x_train, y_train, x_valid, y_valid, x_test, y_test]\n",
    "\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(df_stock_norm, seq_len)\n",
    "print('x_train.shape:', x_train.shape)\n",
    "print('y_train.shape:', y_train.shape)\n",
    "print('x_valid.shape:', x_valid.shape)\n",
    "print('y_valid.shape:', y_valid.shape)\n",
    "print('x_test.shape:', x_test.shape)\n",
    "print('y_test.shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "We will build four different models – Basic RNN Cell, Basic LSTM Cell, LSTM Cell with peephole connections and GRU cell. Please remember you can run one model at a time. I am putting everything into one coding. Whenever you run one model, make sure you put the other model as a comment or else your results will be wrong and python might throw an error.\n",
    "\n",
    "## Parameters, Placeholders & Variables\n",
    "We will first fix the Parameters, Placeholders & Variables to building any model. The Artificial Neural Network starts with placeholders. We need two placeholders in order to fit our model: X contains the network’s inputs (features of the stock (OHLC) at time T = t) and Y the network’s output (Price of the stock at T+1). The shape of the placeholders corresponds to None, n_inputs with [None] meaning that the inputs are a 2-dimensional matrix and the outputs are a 1-dimensional vector. It is crucial to understand which input and output dimensions the neural net needs in order to design it properly. We define the variable batch size as 50 that controls the number of observations per training batch. We stop the training network when epoch reaches 100 as we have given epoch as 100 in our parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters & placeholders\n",
    "n_steps = seq_len-1\n",
    "n_inputs = 4\n",
    "n_neurons = 200\n",
    "n_outputs = 4\n",
    "n_layers = 2\n",
    "learning_rate = 0.001\n",
    "batch_size = 50\n",
    "n_epochs = 100\n",
    "train_set_size = x_train.shape[0]\n",
    "test_set_size = x_test.shape[0]\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing the network architecture\n",
    "Before we proceed, we have to write the function to run the next batch for any model. Then we will write the layers for each model separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the next batch\n",
    "index_in_epoch = 0;\n",
    "perm_array  = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(perm_array)\n",
    "\n",
    "def get_next_batch(batch_size):\n",
    "    global index_in_epoch, x_train, perm_array   \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size \n",
    "    if index_in_epoch > x_train.shape[0]:\n",
    "        np.random.shuffle(perm_array) # shuffle permutation array\n",
    "        start = 0 # start next epoch\n",
    "        index_in_epoch = batch_size     \n",
    "    end = index_in_epoch\n",
    "    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please remember you need to put the other models as a comment whenever you running one particular model. Here we are running only RNN basic so I kept all others as a comment. You can run one model after another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN \n",
    "layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.elu)\n",
    "         for layer in range(n_layers)]\n",
    "# LSTM  \n",
    "#layers = [tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons, activation=tf.nn.elu)\n",
    "#        for layer in range(n_layers)]\n",
    "\n",
    "#LSTM with peephole connections\n",
    "#layers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons, \n",
    "#                                  activation=tf.nn.leaky_relu, use_peepholes = True)\n",
    "#          for layer in range(n_layers)]\n",
    "\n",
    "#GRU \n",
    "#layers = [tf.contrib.rnn.GRUCell(num_units=n_neurons, activation=tf.nn.leaky_relu)\n",
    "#          for layer in range(n_layers)] \n",
    "                                                                  \n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
    "stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) \n",
    "stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\n",
    "outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\n",
    "outputs = outputs[:,n_steps-1,:] # keep only last output of sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function\n",
    "We use cost function to optimize the model. The cost function is used to generate a measure of deviation between the network’s predictions and the actual observed training targets. For regression problems, the mean squared error (MSE) function is commonly used. MSE computes the average squared deviation between predictions and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-77a21c922dd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Cost function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "# Cost function\n",
    "loss = tf.reduce_mean(tf.square(outputs - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "The optimizer takes care of the necessary computations that are used to adapt the network’s weight and bias variables during training. Those computations invoke the calculation of gradients that indicate the direction in which the weights and biases have to be changed during training in order to minimize the network’s cost function. The development of stable and speedy optimizers is a major field in neural network and deep learning research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model we use **Adam** (Adaptive Moment Estimation) Optimizer, which is an extension of the stochastic gradient descent, is one of the default optimizers in deep learning development.\n",
    "\n",
    "\n",
    "# Fitting the neural network model & prediction\n",
    "Now we need to fit the model that we have created to our train datasets. After having defined the placeholders, variables, initializers, cost functions and optimizers of the network, the model needs to be trained. Usually, this is done by mini batch training. During mini batch training random data samples of n = batch_size are drawn from the training data and fed into the network.\n",
    "\n",
    "The training dataset gets divided into n / batch_size batches that are sequentially fed into the network. At this point the placeholders X and Y come into play. They store the input and target data and present them to the network as inputs and targets.\n",
    "\n",
    "A sampled data batch of X flows through the network until it reaches the output layer. There, TensorFlow compares the model’s predictions against the actual observed targets Y in the current batch. Afterwards, TensorFlow conducts an optimization step and updates the network parameters, corresponding to the selected learning scheme. After having updated the weights and biases, the next batch is sampled and the process repeats itself.\n",
    "\n",
    "The procedure continues until all batches have been presented to the network. One full sweep over all batches is called an epoch.\n",
    "\n",
    "The training of the network stops once the maximum number of epochs is reached or another stopping criterion defined by the user applies. We stop the training network when epoch reaches 100 as we have given epoch as 100 in our parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "with tf.Session() as sess: \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for iteration in range(int(n_epochs*train_set_size/batch_size)):\n",
    "        x_batch, y_batch = get_next_batch(batch_size) # fetch the next training batch \n",
    "        sess.run(training_op, feed_dict={X: x_batch, y: y_batch}) \n",
    "        if iteration % int(5*train_set_size/batch_size) == 0:\n",
    "            mse_train = loss.eval(feed_dict={X: x_train, y: y_train}) \n",
    "            mse_valid = loss.eval(feed_dict={X: x_valid, y: y_valid}) \n",
    "            print('%.2f epochs: MSE train/valid = %.6f/%.6f'%(\n",
    "                iteration*batch_size/train_set_size, mse_train, mse_valid))\n",
    "# Predictions\n",
    "    y_test_pred = sess.run(outputs, feed_dict={X: x_test})\n",
    "    \n",
    "#checking prediction output nos \n",
    "y_test_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s compare between our target and prediction. I put both target (y_test) & prediction (y_test_pred) closing price in one data frame named as “comp”. Now I put both prices in one graph, let see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-d04e2e48bdd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# ploting the graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcomp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Column1'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Column2'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_test_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Column1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'blue'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Target'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Column2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'black'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Prediction'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# ploting the graph\n",
    "comp = pd.DataFrame({'Column1':y_test[:,3],'Column2':y_test_pred[:,3]})\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(comp['Column1'], color='blue', label='Target')\n",
    "plt.plot(comp['Column2'], color='black', label='Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting output\n",
    "Now we can see, the results are not bad. The prediction values are exactly the same as the target value and moving in the same direction as we expect. You can check the difference between these two and compare the results in various ways & optimize the model before you build your trading strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The objective of this project is to make you understand how to build different neural network model like RNN, LSTM & GRU in python tensor flow and predicting stock price. You can optimize this model in various ways and build your own trading strategy to get a good strategy return considering **Hit Ratio**, **draw down** etc.\n",
    "\n",
    "Another important factor, we have used daily prices in this model so the data points are really less only 5,640 data points. My advice is to use more than 100,000 data points (use minute or tick data) for training the model when you are building Artificial Neural Network or any other Deep Learning model that will be most effective.\n",
    "\n",
    "Now you can build your own trading strategy using the power and intelligence of your machines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
